\studentName{Uğurcan Özalp}
\advisorName{Prof.~Dr.~Ömür Uğur}
% Your Department: 
% Scientific Computing, 
% Financial Mathematics, 
% Cryptography, or 
% Actuarial Sciences
\departmentName{Scientific Computing}
% Type of the Document:
% Preprint # may not work as expected (to-do)
% Qualification in PhD
% Report for Thesis Monitoring Committee
% Term Project
% Draft of a Manuscript
\paperType{Draft of a Manuscript}


\paperTitle{%
	Bipedal Robot Walking by Reinforcement Learning in Partially Observed Environment
}

\paperAuthor{%
  U. \"{O}zalp\footnote{%
    Middle East Technical University, Institute of Applied
    Mathematics, 06800 \c{C}ankaya, Ankara, Turkey. \hfill
  \emph{E-Mail}: \texttt{ugurcan.ozalp@metu.edu.tr}},
	% possibly your advisor and co-advisor
  \"{O}. U\u{g}ur\footnote{Middle East Technical University, Institute of Applied
  Mathematics, 06800 \c{C}ankaya, Ankara, Turkey. \hfill
  \emph{E-Mail}: \texttt{ougur@metu.edu.tr}}
}
% abstract should not be large to fit in the title/front page
\paperAbstract{%
Deep Reinforcement Learning methods on mechanical control have been successfully applied in many environments and used instead of traditional optimal and adaptive control methods for some complex problems. 
However, Deep Reinforcement Learning algorithms do still have some challenges. 
One is to control on partially observable environments. 
When an agent is not informed well of the environment, it must recover information from the past observations. 
In this thesis, walking of Bipedal Walker Hardcore (OpenAI GYM) environment, 
which is partially observable, 
is studied by two continuous actor-critic reinforcement learning algorithms; Twin Delayed Deep Determinstic Policy Gradient and Soft Actor Critic.
Several neural architectures are implemented. 
The first one is Residual Feed Forward Neural Network under the observable environment assumption, 
while the second and the third ones are Long Short Term Memory and Transformer using observation history as input to recover the hidden information due to the partially observable environment. 
}

\paperKeywords{%
  deep reinforcement learning, partial observability, robot control, actor-critic methods, long short term memory, transformer
}

\paperDate{July 2021}

