\section{Discussion}
These results are not enough to conclude on a superior neural network for all RL problems, because there are other factors such as DRL algorithm, number of episodes, network size etc. 
However, networks are designed to have similar sizes and good model requires to converge in less episodes. 
As a result, LSTM is superior to Transformer for our environment. 
In addition, it is possible to conclude that Transformers can be an option for partially observed RL problems.
Note that this is valid where layer normalization is applied before multihead attention and feed-forward layers \cite{xiong_layer_2020} as opposed to vanilla transformer proposed in \cite{vaswani_attention_2017}. 

Another result is that incorprating past observations did improve performance significantly since environment is partially observable.
Especially increasing history length keeps increasing performance for both LSTM and Transformer models. 
To address this issue better, we designed Transformer and RFFNN networks to be almost same once multi-head attention layer is removed. 
We observed significant performance gains as observation history is used as input to controller. 

The environment is a difficult one. 
There are really few available models with solution~\cite{noauthor_gymleaderboard_2021}. 
Apart from neural networks, there are other factors affecting performance such as RL algorithm, rewarding, exploration etc. 
In this work, all of them are adjusted such that the environment becomes solvable. 
Time frequency is reduced for sample efficiency and speed. 
Also, the agent is not informed for the terminal state when it reaches time limit. 
Lastly, punishment of falling reduced, so the agent is allowed to learn by mistakes. 
Those modifications are probably another source of our high performance. 

As RL algorithm, TD3 is selected first, since it is suitable for continuous RL. 
Ornstein-Uhlenbeck noise is used for better exploration since it has momentum, and variance is reduced by time to make agent learn small actions well in later episodes. 
However, we cannot feed 12 observation history to sequential models since TD3 failed to learn in that case.
In addition, SAC is used for learning along with TD3. 
Results are better compared to those of TD3. 
SAC policy maximizes randomness (entropy) if agent cannot get sufficient reward and this allows the agent to decide where/when to explore more or less. 
This way, SAC handles the sparse rewards from the environment better than TD3. 
