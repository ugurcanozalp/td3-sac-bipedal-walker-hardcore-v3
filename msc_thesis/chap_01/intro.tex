\chapter{INTRODUCTION}
\label{chap:intro}

%Artificial intelligence is the ability of a computer program 
%or a machine to think and learn like natural intelligence 
%performed by humans and animals. 
%One way is to create an intellgent agent is using pattern detection  methods on data and use it to make predictions on unseen data. 
%This approach is called Machine Learning. 

Humans and animals exhibit several different behaviours in terms of 
interaction with environment, such as utterance and movement. 
Their behavior is based on past experience, the situation they are in  and their objective. 
Like humans and animals, an intelligent agent is expected to take 
action according to its perception based on some objective. 
A major challenge in Machine Learning (ML) to create agents that will 
act more natural and humanlike. 
As a subfield of ML, Reinforcement Learning (RL) allows an 
agent to learn how to control (or act) itself in different situations by interacting with the environment. 
In RL, environment is modeled to give reward (or punishment) to agent 
according to environmental state and agent actions, and agent focuses
on learning to predict what actions will lead to highest reward 
(or lowest punishment, based on its objective) in the future using past experience. 

Traditional RL algorithms need feature engineering from observations. 
For complex problems, the way to extract features is ambiguous or 
observations are not enough to create a good model. 
As a newer  technique, Deep Neural Networks (DNNs) allows to extract 
high level features from data with large state-space 
(like pixelwise visual, lidar scan, multiple kinematic sensors etc.) and missing  observations. 
Along with recent developments in DNNs, Deep Reinforcement Learning (DRL)
allows an agent to interact with the environment in a more complex way. 
The problem with DRL is the selection of a correct neural network, 
however, there is still no analytical way to design a neural network for all tasks. 
Therefore, neural design is commonly based on trial-error experiments for the particular problems at hand.

Since its discovery, robots have been crucial devices for the human race, whether smart or not. 
Intelligent humanoid and animaloid robots have been developed since early 1980s. 
This type of robots has legs unlike driving robots. 
Since most of the world terrain is unpaved, this type of robots are good alternative to driving robots. 
Locomotion is a major task for such robots. Stable bipedal (2 legged)  walking 
is one of the most challenging problem among the control problems. 
It is hard to create accurate model due to high order of dynamics,  friction and discontinuities. 
Even further, the design of walking controller using traditional methods is difficult due to the same reasons. 
Therefore, for bipedal walking, DRL approach is an easier choice if a simulation environment is available. 

In this thesis, Bipedal Locomotion is investigated through \textit{BipedalWalker-Hardcore-v3} \cite{noauthor_bipedalwalkerhardcore-v2_2021} environment of open source GYM library \cite{brockman_openai_2016} by DRL. 
Our contributions to the related literature may be summarized as follows:
\begin{itemize}
	\item Sequential neural networks are used (LSTM and Transformer) for solution, along with Residual Feed Forward Neural Network.
	\item Twin Delayed Deep Deterministic Policy Gradient (TD3) and Soft Actor-Critic (SAC) algorithms are used and compared.
	\item Reward shaping and small modifications are applied on environment to hinder possible problems.
\end{itemize}

